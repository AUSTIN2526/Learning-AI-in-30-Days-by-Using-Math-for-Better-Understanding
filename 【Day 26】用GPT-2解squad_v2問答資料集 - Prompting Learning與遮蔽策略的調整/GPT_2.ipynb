{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "# 由於GPT-2沒有PAD token所以使用EOS Token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id \n",
    "\n",
    "# 讀取CSV檔案並只選取指定的3個欄位\n",
    "df = pd.read_csv('squad2.0_converted.csv', usecols=['context', 'question', 'answer'])\n",
    "df = df.fillna('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加入Prompt\n",
    "df['context'] = '### Context:\\n' + df['context']\n",
    "df['question'] = '\\n### Question:\\n' + df['question']\n",
    "\n",
    "# 在答案後方加入EOS token表示文本結尾\n",
    "df['answer'] = '\\n### Answer:\\n' + df['answer'] + tokenizer.eos_token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(df, train_size=0.8, random_state=46, shuffle=True)\n",
    "print(train_df['context'][0], end='')\n",
    "print(train_df['question'][0], end='')\n",
    "print(train_df['answer'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SquadDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.dataframe.iloc[index]\n",
    "        return item['context'], item['question'], item['answer']\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    # 將文本進行分詞\n",
    "    def tokenize_data(self, texts, max_length=512):\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            list(texts),\n",
    "            truncation=True,\n",
    "            padding='longest',\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return tokenized_inputs.input_ids, tokenized_inputs.attention_mask\n",
    "\n",
    "    # 定義數據加載過程中的數據整理方法\n",
    "    def collate_fn(self, batch):\n",
    "        contexts, questions, answers = zip(*batch)\n",
    "        \n",
    "        # 輸入和答案\n",
    "        question_ids, question_attention_mask = self.tokenize_data(questions)\n",
    "        answer_ids, answer_attention_mask = self.tokenize_data(answers)\n",
    "        context_ids, context_attention_mask = self.tokenize_data(contexts, max_length=1024-answer_ids.shape[1]-question_ids.shape[1])\n",
    "       \n",
    "\n",
    "        # 模型的輸入 = context_ids + question_ids + answer_ids\n",
    "        combined_input_ids = torch.cat((context_ids, question_ids, answer_ids), dim=-1)\n",
    "        # 模型的MASK = context_attention_mask + question_attention_mask + answer_attention_mask\n",
    "        combined_attention_mask = torch.cat((context_attention_mask, question_attention_mask, answer_attention_mask), dim=-1)\n",
    "\n",
    "        # 模型的標籤 = context_ids * [-100] + question_ids * [-100] + answer_ids + [EOS] \n",
    "        context_ignore_mask = torch.full((context_ids.shape[0], context_ids.shape[-1]), -100) # 產生context_ids * [-100]\n",
    "        question_ignore_mask = torch.full((question_ids.shape[0], question_ids.shape[-1]), -100) # 產生question_ids * [-100]\n",
    "        answer_ignore_indices = (answer_attention_mask == 0) # 找出Answer的[PAD] idx\n",
    "        answer_ids[answer_ignore_indices] = -100 # 將Answer為[PAD]的部分轉換成-100\n",
    "        combined_answers = torch.cat((context_ignore_mask, question_ignore_mask, answer_ids), dim=-1) #context_ignore_mask + question_ignore_mask + answer_ids\n",
    "\n",
    "        return {\n",
    "            'input_ids': combined_input_ids,\n",
    "            'attention_mask': combined_attention_mask,\n",
    "            'labels': combined_answers,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立資料集\n",
    "trainset = SquadDataset(train_df, tokenizer)\n",
    "validset = SquadDataset(valid_df, tokenizer)\n",
    "\n",
    "# 創建 DataLoader\n",
    "train_loader = DataLoader(trainset, batch_size=4, shuffle=True, collate_fn=trainset.collate_fn)\n",
    "valid_loader = DataLoader(validset, batch_size=4, shuffle=True, collate_fn=validset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from transformers import get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# 訓練設置\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=len(train_loader) * 0.2, \n",
    "        num_training_steps=len(train_loader) * 10, \n",
    "        num_cycles=1, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Trainer import Trainer\n",
    "trainer = Trainer(\n",
    "    epochs=10, \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader,\n",
    "    model=model, \n",
    "    optimizer=[optimizer],\n",
    "    scheduler=[scheduler],\n",
    "    early_stopping=3,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, tokenizer, context, question, device):\n",
    "    # 準備輸入數據\n",
    "    inference_data = f\"{context}{question}\\n### Answer:\\n\"\n",
    "    # 進行編碼和截斷\n",
    "    try:\n",
    "        inputs = tokenizer(inference_data, max_length=1024, truncation=True, return_tensors='pt').to(device)\n",
    "        # 禁用梯度計算，進行生成\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=1024, do_sample=False)\n",
    "        \n",
    "        # 解碼並提取答案部分\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        answer = generated_text.split('\\n### Answer:\\n')[1].strip()\n",
    "        \n",
    "        return answer\n",
    "    except:\n",
    "        return 'Error'\n",
    "\n",
    "# 載入模型和設定評估模式\n",
    "model.load_state_dict(torch.load('model.ckpt'))\n",
    "model.eval()\n",
    "\n",
    "# 指定要進行推理的索引\n",
    "idx = 7\n",
    "\n",
    "# 準備推理資料\n",
    "context = valid_df['context'].values[idx]\n",
    "question = valid_df['question'].values[idx]\n",
    "answer = valid_df['answer'].values[idx]\n",
    "\n",
    "\n",
    "# 進行推理\n",
    "model.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "model_answer = inference(model, tokenizer, context, question, device)\n",
    "\n",
    "\n",
    "# 輸出原始上下文、問題、真實答案和模型生成的答案\n",
    "print(f\"{context}\")\n",
    "print(f\"{question}\")\n",
    "print(f\"{answer.split(tokenizer.eos_token)[0]}\")\n",
    "print(\"\\n### Model Answer:\\n\" + model_answer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
