{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def read_csv_data(data_path):\n",
    "    source, target = [], []\n",
    "    for file_name in os.listdir(data_path):\n",
    "        df = pd.read_csv(f'{data_path}/{file_name}')\n",
    "        src, tgt = df['text'].values, df['summary'].values\n",
    "        source.extend(src)\n",
    "        target.extend(tgt)\n",
    "    return source, target\n",
    "    \n",
    "x_train_data, y_train_data = read_csv_data('news/train')\n",
    "x_test, y_test = read_csv_data('news/test')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class SummaryeDataset(Dataset):\n",
    "    def __init__(self, x, y, tokenizer):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def collate_fn(self, batch):    \n",
    "        batch_x, batch_y = zip(*batch)\n",
    "        src = self.tokenizer(batch_x, max_length=256, truncation=True, padding=\"longest\", return_tensors='pt')\n",
    "        tgt = self.tokenizer(batch_y, max_length=256, truncation=True, padding=\"longest\", return_tensors='pt')\n",
    "        src = {f'src_{k}':v for k, v in src.items()}\n",
    "        tgt = {f'tgt_{k}':v for k, v in tgt.items()}\n",
    "\n",
    "        return {**src, **tgt}\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_data, y_train_data, train_size=0.8, random_state=46, shuffle=True) \n",
    "\n",
    "trainset = SummaryeDataset(x_train, y_train, tokenizer)\n",
    "validset = SummaryeDataset(x_valid, y_valid, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size = 32, shuffle = True, num_workers = 0, pin_memory = True, collate_fn=trainset.collate_fn)\n",
    "valid_loader = DataLoader(validset, batch_size = 32, shuffle = True, num_workers = 0, pin_memory = True, collate_fn=validset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size, dropout, maxlen=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(maxlen, emb_size)\n",
    "        position = torch.arange(0, maxlen, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, emb_size, 2).float() * (-torch.log(torch.tensor(10000.0)) / emb_size))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.src_embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.tgt_embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=0.1)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, # 對應的嵌入層維度跟emb_size相同大小\n",
    "            nhead=nhead,     # Muti-head Attention head數量\n",
    "            num_encoder_layers=num_encoder_layers, # 要幾個Encoder進行運算\n",
    "            num_decoder_layers=num_decoder_layers, # 要幾個Decoder進行運算\n",
    "            dim_feedforward=dim_feedforward,       # Layer Norm輸出維度\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # 用於生成最終輸出的線性層\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        src_ids = kwargs['src_input_ids']\n",
    "        tgt_ids = kwargs['tgt_input_ids']\n",
    "        src_emb, tgt_emb = self.embedding_step(src_ids, tgt_ids)\n",
    "\n",
    "        src_key_padding_mask = (kwargs['src_attention_mask'] == 0)\n",
    "        tgt_key_padding_mask = (kwargs['tgt_attention_mask'] == 0)\n",
    "\n",
    "        src_mask = torch.zeros((src_emb.shape[1], src_emb.shape[1]), device=src_ids.device.type).type(torch.bool)\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt_emb)\n",
    "\n",
    "        # 將嵌入通過transformer模型\n",
    "        outs = self.transformer(\n",
    "            src_emb, tgt_emb, \n",
    "            src_mask=src_mask, \n",
    "            tgt_mask=tgt_mask, \n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask, \n",
    "            memory_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "\n",
    "        logits = self.fc(outs)\n",
    "\n",
    "        tgt_ids_shifted = tgt_ids[:, 1:].reshape(-1)\n",
    "        logits = logits[:, :-1].reshape(-1, logits.shape[-1])\n",
    "        loss = self.criterion(logits, tgt_ids_shifted)\n",
    "\n",
    "        return loss, logits\n",
    "\n",
    "    def embedding_step(self, src, tgt):\n",
    "        src_emb = self.src_embedding(src)\n",
    "        tgt_emb = self.tgt_embedding(tgt)\n",
    "        \n",
    "        return self.positional_encoding(src_emb), self.positional_encoding(tgt_emb)\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, tgt_emb):\n",
    "        sz = tgt_emb.shape[1]\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask.to(tgt_emb.device.type)\n",
    "    \n",
    "    def generate(self, max_length=50, cls_token_id=101, sep_token_id=102, **kwargs):\n",
    "        src_input_ids = kwargs['input_ids']\n",
    "        src_attention_mask = kwargs['attention_mask']\n",
    "\n",
    "        # 先嵌入源序列\n",
    "        src_emb = self.positional_encoding(self.src_embedding(src_input_ids))\n",
    "        src_key_padding_mask = (src_attention_mask == 0)\n",
    "\n",
    "        # 初始化目標序列，開始符號 (BOS)\n",
    "        tgt_input_ids = torch.full((src_input_ids.size(0), 1), cls_token_id, dtype=torch.long).to(src_input_ids.device)\n",
    "        for _ in range(max_length):\n",
    "            tgt_emb = self.tgt_embedding(tgt_input_ids)\n",
    "            tgt_emb = self.positional_encoding(tgt_emb)\n",
    "\n",
    "            # Transformer 前向傳播\n",
    "            outs = self.transformer(\n",
    "                src_emb, tgt_emb, \n",
    "                src_key_padding_mask=src_key_padding_mask, \n",
    "                memory_key_padding_mask=src_key_padding_mask\n",
    "            )\n",
    "            logits = self.fc(outs)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(1)\n",
    "            tgt_input_ids = torch.cat([tgt_input_ids, next_token], dim=1)\n",
    "\n",
    "            # 停止條件: 如果生成的序列中包含了結束符號 (EOS)\n",
    "            if next_token.item() == sep_token_id:\n",
    "                break\n",
    "\n",
    "        return tgt_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from transformers import get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "from Trainer import Trainer\n",
    "\n",
    "# 設定模型\n",
    "model = Seq2SeqTransformer(\n",
    "    vocab_size=len(tokenizer),\n",
    "    emb_size=512,\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dim_feedforward=2048\n",
    ")\n",
    "\n",
    "# 優化器與排成器\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=len(train_loader), \n",
    "        num_training_steps=len(train_loader) * 100, \n",
    "        num_cycles=1, \n",
    ")\n",
    "\n",
    "# 訓練模型\n",
    "trainer = Trainer(\n",
    "    epochs=100, \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader, \n",
    "    model=model, \n",
    "    optimizer=[optimizer],\n",
    "    scheduler=[scheduler]\n",
    ")\n",
    "trainer.train(show_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.load_state_dict(torch.load('model.ckpt')).to(device)\n",
    "model.eval()\n",
    "idx = 7778\n",
    "input_data = tokenizer(x_test[idx], max_length=1024, truncation=True, padding=\"longest\", return_tensors='pt').to(device)\n",
    "generated_ids = model.generate(**input_data, max_len=50)\n",
    "\n",
    "print('輸入文字:\\n', x_test[idx])\n",
    "print('目標文字:\\n', y_test[idx])\n",
    "print('模型文字:\\n', tokenizer.decode(generated_ids[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
