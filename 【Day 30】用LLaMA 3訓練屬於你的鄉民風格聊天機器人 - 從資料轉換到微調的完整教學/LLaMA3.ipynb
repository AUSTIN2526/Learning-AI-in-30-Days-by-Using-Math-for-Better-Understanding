{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取Tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "    trust_remote_code=True,\n",
    "    add_special_tokens=False\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "system_format = {\"role\": \"system\", \"content\": '這是系統指令'}\n",
    "question_format = {\"role\": \"user\", \"content\": '這是用戶的輸入'}\n",
    "answer_format = {\"role\": \"assistant\", \"content\": '這是模型回復'}\n",
    "\n",
    "chat_format = tokenizer.apply_chat_template([system_format, question_format, answer_format])\n",
    "print(tokenizer.decode(chat_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "quantization_params = {\n",
    "            'load_in_4bit': True,\n",
    "            'bnb_4bit_quant_type': \"nf4\",\n",
    "            'bnb_4bit_use_double_quant': True,\n",
    "            'bnb_4bit_compute_dtype': torch.bfloat16\n",
    "        }\n",
    "bnb_config = BitsAndBytesConfig(**quantization_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "device_map = {\"\": Accelerator().local_process_index}\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        'meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=device_map,\n",
    "        use_cache=False,\n",
    "    )\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "peft_params = {\n",
    "            'r': 32,        \n",
    "            'target_modules': [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "            'lora_dropout': 0.1,\n",
    "            'task_type': \"CAUSAL_LM\",\n",
    "        }\n",
    "peft_config = LoraConfig(**peft_params)\n",
    "\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_utils import unwrap_model\n",
    "\n",
    "def activate_neftune(model, neftune_noise_alpha = 5):\n",
    "        unwrapped_model = unwrap_model(model)\n",
    "        embeddings = unwrapped_model.base_model.model.get_input_embeddings()\n",
    "        embeddings.neftune_noise_alpha = neftune_noise_alpha # 讓Embedding層的__init__多一個neftune_noise_alpha參數\n",
    "        # hook embedding layer\n",
    "        hook_handle = embeddings.register_forward_hook(neftune_post_forward_hook)\n",
    "        \n",
    "        return model\n",
    "        \n",
    "def neftune_post_forward_hook(module, input, output):\n",
    "    # 公式來源:https://github.com/neelsjain/NEFTune\n",
    "    # 論文網址:https://arxiv.org/abs/2310.05914\n",
    "    if module.training: # 讓他再訓練時有用而已\n",
    "        # 實現NEFtune公式\n",
    "        dims = torch.tensor(output.size(1) * output.size(2))\n",
    "        mag_norm = module.neftune_noise_alpha / torch.sqrt(dims) # 這裡的neftune_noise_alpha就是在__init__的參數\n",
    "        output = output + torch.zeros_like(output).uniform_(-mag_norm, mag_norm)\n",
    "            \n",
    "    return output\n",
    "model = activate_neftune(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def transform_format(questions, answers, system='你是一個zh-tw版本的聊天機器人'):\n",
    "    context = []\n",
    "    for q, a in zip(questions, answers):\n",
    "        system_format = {\"role\": \"system\", \"content\": system}\n",
    "        question_format = {\"role\": \"user\", \"content\": q}\n",
    "        answer_format = {\"role\": \"assistant\", \"content\": a}\n",
    "        context.append([system_format, question_format, answer_format])\n",
    "    return context\n",
    "\n",
    "# 讀取CSV檔案\n",
    "df = pd.read_csv('Gossiping-QA-Dataset-2_0.csv')\n",
    "\n",
    "# 提取問題和答案的列表\n",
    "questions = df['question'].tolist()[:5000]\n",
    "answers = df['answer'].tolist()[:5000]\n",
    "\n",
    "# 轉換格式\n",
    "formatted_context = transform_format(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 定義自定義 Dataset\n",
    "class PTTDataset(Dataset):\n",
    "    def __init__(self, formatted_context, tokenizer):\n",
    "        self.formatted_context = formatted_context\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.formatted_context[index]\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.formatted_context)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        formatted_contexts = self.tokenizer.apply_chat_template(batch, padding=True, return_dict=True, max_length=8192, return_tensors='pt', truncation=True)\n",
    "        attention_mask = formatted_contexts['attention_mask']\n",
    "        labels = formatted_contexts['input_ids'].clone()\n",
    "        labels[attention_mask == 0] = -100\n",
    "        formatted_contexts['labels'] = labels\n",
    "        return formatted_contexts\n",
    "\n",
    "# 建立資料集\n",
    "trainset = PTTDataset(formatted_context, tokenizer)\n",
    "validset = PTTDataset(formatted_context, tokenizer)\n",
    "\n",
    "# 創建 DataLoader\n",
    "train_loader = DataLoader(trainset, batch_size=4, shuffle=True, collate_fn=trainset.collate_fn)\n",
    "valid_loader = DataLoader(validset, batch_size=4, shuffle=True, collate_fn=validset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from transformers import get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "from Trainer import Trainer\n",
    "\n",
    "# 訓練設置\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=len(train_loader) * 0.2, \n",
    "        num_training_steps=len(train_loader) * 10, \n",
    "        num_cycles=1, \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    epochs=10, \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader,\n",
    "    model=model, \n",
    "    optimizer=[optimizer],\n",
    "    scheduler=[scheduler],\n",
    "    early_stopping=3,\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
